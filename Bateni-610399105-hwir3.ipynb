{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our task is to design an IR system on the given dataset which can be found [here](https://drive.google.com/drive/folders/1h7AzOoPrbaEx2ApcGq-MO22quMcOi40H?usp=share_link. \"link to dataset\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1: dataset input\n",
    "reading the files from the dataset. In order to do this step we use the OS library and list all the files available in the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jerry Decided To Buy a Gun.txt\n",
      "Rentals at the Oceanside Community.txt\n",
      "Gasoline Prices Hit Record High.txt\n",
      "Cloning Pets.txt\n",
      "Crazy Housing Prices.txt\n",
      "Man Injured at Fast Food Place.txt\n",
      "A Festival of Books.txt\n",
      "Food Fight Erupted in Prison.txt\n",
      "Better To Be Unlucky.txt\n",
      "Sara Went Shopping.txt\n",
      "Freeway Chase Ends at Newsstand.txt\n",
      "Trees Are a Threat.txt\n",
      "A Murder-Suicide.txt\n",
      "Happy and Unhappy Renters.txt\n",
      "Pulling Out Nine Tons of Trash.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#Path to directory\n",
    "directory_path = \"./docs\"\n",
    "\n",
    "#Getting a list of files in the directory\n",
    "file_names = os.listdir(directory_path)\n",
    "\n",
    "#DEBUG: checking the list\n",
    "print(*file_names,sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the names, we can start by reading the content of every file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of document content\n",
    "file_content = []\n",
    "\n",
    "for i in range(len(file_names)):\n",
    "    file_names[i] = directory_path+\"/\"+file_names[i]\n",
    "    #with open(directory_path+\"/\"+file_name, 'r', encoding='cp1252') as file:\n",
    "    #    file_content.append(file.readlines())\n",
    "    #    #print(file_content[-1])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1.5: blocked sort-based indexing\n",
    "We are going to implement the BSBI algorithm as defined in the slides. In order to do so, we define our blocks and then load the documents of each block. After making the inverted index of each block, we merge them all in one universal inverted index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "block_size = 3\n",
    "\n",
    "def encode_gamma(num):\n",
    "    # Get binary representation, excluding the leading '0b'\n",
    "    binary_repr = bin(num + 1)[3:]\n",
    "    gamma_code = '0' * (len(binary_repr) - 1) + binary_repr\n",
    "    return gamma_code\n",
    "\n",
    "def write_block_to_disk(sorted_terms, in_memory_index, block_num):\n",
    "    # Write the in-memory index for a block to disk\n",
    "    block_filename = f'block_{block_num}.json'\n",
    "    with open(block_filename, 'w') as file:\n",
    "        encoded_index = {term: [encode_gamma(gap) for gap in gaps] for term, gaps in in_memory_index.items()}\n",
    "        json.dump({'sorted_terms': sorted_terms, 'index': encoded_index}, file)\n",
    "\n",
    "def merge_blocks_on_disk():\n",
    "    # Merge the indices from all blocks\n",
    "    merged_index = defaultdict(list)\n",
    "    block_file_names = [file for file in os.listdir() if file.startswith('block_')]\n",
    "\n",
    "    for block_file in block_file_names:\n",
    "        with open(block_file, 'r') as file:\n",
    "            block_data = json.load(file)\n",
    "            for term in block_data['sorted_terms']:\n",
    "                gaps = [int(gap, 2) for gap in block_data['index'][term]]\n",
    "                merged_index[term].extend(gaps)\n",
    "\n",
    "    return merged_index\n",
    "\n",
    "def write_merged_index_to_disk(merged_index):\n",
    "    # Write the merged index to disk\n",
    "    with open('inverted_index.json', 'w') as file:\n",
    "        encoded_index = {term: [encode_gamma(gap) for gap in gaps] for term, gaps in merged_index.items()}\n",
    "        json.dump(encoded_index, file)\n",
    "\n",
    "def BSBI(block_size):\n",
    "\n",
    "    # Create File Blocks\n",
    "    blocks = [file_names[i:i + block_size] for i in range(0, len(file_names), block_size)]\n",
    "\n",
    "    # Block Building and In-Memory Sorting\n",
    "    block_content = []\n",
    "    for block in blocks:\n",
    "        in_memory_index = defaultdict(list)\n",
    "        prev_doc_id = 0\n",
    "\n",
    "        block_content.append([])\n",
    "        for doc_path in block:\n",
    "            with open(doc_path, 'r', encoding='cp1252') as doc_file:\n",
    "                doc_content = doc_file.readlines()\n",
    "                block_content[-1].append(doc_content)\n",
    "\n",
    "blocks = []\n",
    "block_content = []\n",
    "BSBI(block_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2: tokenizing\n",
    "Now that we have the text in our program we should start by tokenizing the text.\n",
    "In order to do this we're going to use the nltk library in python and to make our job easier we're going to tokenize words separated by Space, Comma, and dash.\n",
    "\n",
    "Examples:\n",
    "1. I live ...\n",
    "2. Student, jason, ...\n",
    "3. 30-year-old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# A list for tokenized texts\n",
    "block_file_content_tokenized = []\n",
    "\n",
    "# Define a regular expression pattern to match words separated by Space, Comma, and Dash\n",
    "#pattern = r'\\w+|\\$[\\d\\.]+'\n",
    "pattern = r'\\d{1,4}(?:,\\d{3})*(?:\\.\\d+)?|\\w+'\n",
    "#pattern = r'[A-Za-z0-9]+.*[A-Za-z0-9]+.*([+-]?(?=\\.\\d|\\d)(?:\\d+)?(?:\\.?\\d*))(?:[Ee]([+-]?\\d+))?.*([0-9]+(,[0-9]+)+)'\n",
    "\n",
    "for i in range(len(blocks)):\n",
    "    #Change all input data into tokens\n",
    "    block_file_content_tokenized.append([])\n",
    "    for file in block_content[i]:\n",
    "        # Use regex_tokenize with the defined pattern\n",
    "        block_file_content_tokenized[i].append(RegexpTokenizer(pattern).tokenize(file[0]))\n",
    "        print(block_file_content_tokenized[i][-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 3: lowercasing\n",
    "We now want to lowercase our tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(blocks)):\n",
    "    #lowercasing each token individually\n",
    "    for i in range(len(block_file_content_tokenized[k])):\n",
    "        for j in range(len(block_file_content_tokenized[k][i])):\n",
    "            block_file_content_tokenized[k][i][j] = block_file_content_tokenized[k][i][j].lower()\n",
    "        print(block_file_content_tokenized[k][i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 4: stopword removal\n",
    "Now we want to remove the stopwords present in our text. To do this task we use the library of stopwords in nltk library. Please note that in order to be able to perform positional queries, we will not delete stopwords, just replace them with empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'after', 'had', 'on', 'myself', \"shan't\", \"you're\", 're', 'doesn', 'each', 'once', 'will', 'there', 'should', 'weren', 'ain', 'd', 'are', 'have', 'very', 'an', 'you', 'do', 'why', 'me', 'into', 'below', 'shan', \"haven't\", 'those', 'shouldn', 'no', 'up', \"doesn't\", 'about', \"hasn't\", 'too', 'isn', 'under', 'other', 'my', 'more', 'down', 's', 'itself', 'it', 'these', \"you'll\", 'because', 'so', 'if', 'she', 'were', 'can', 'the', 'while', 'haven', 'does', 'mightn', \"don't\", 'we', 'and', 'any', 'm', 'how', 'nor', 'hers', 'wouldn', 'yourselves', 'what', 'our', 'its', 'from', 'ma', 'against', 'ours', \"wouldn't\", 'wasn', 'then', 'just', \"weren't\", 'all', 'i', 'during', \"mustn't\", 'before', 'ourselves', 'here', 'as', 'not', 'most', 'who', 'same', \"mightn't\", \"you've\", \"didn't\", \"aren't\", 'in', 'than', 'aren', 'but', 'when', 'they', 'your', 'of', 'his', \"isn't\", \"should've\", 'o', 'hasn', 'further', \"hadn't\", 'did', 'this', \"she's\", 'where', 'theirs', 'be', 'hadn', 'now', 'out', \"won't\", \"that'll\", 'which', 'herself', 'her', 'am', 'a', 'that', 'their', 'to', 'through', 'didn', 'has', 'y', 'by', 'own', 'he', 'having', 'over', \"shouldn't\", 'doing', 'mustn', 'themselves', 'only', 'don', 'few', \"wasn't\", 'some', 'yours', 'whom', 'being', 'again', 'between', 'him', 'them', 'with', \"couldn't\", 'been', \"needn't\", 'was', 'won', 'couldn', 'yourself', 'is', 'off', 'll', 'at', 'such', 'above', 'both', 'himself', 'or', \"you'd\", 't', 'needn', 'for', 'until', \"it's\", 've'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Setting the stopword language\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(stop_words)\n",
    "\n",
    "for k in range(len(blocks)):\n",
    "    for i in range(len(block_file_content_tokenized[k])):\n",
    "        j = 0\n",
    "        #print(file_content_tokenized[i])\n",
    "        while j < len(block_file_content_tokenized[k][i]):\n",
    "            if block_file_content_tokenized[k][i][j] in stop_words:\n",
    "                #print(\"got here with \", file_content_tokenized[i][j])\n",
    "                #file_content_tokenized[i] = file_content_tokenized[i][:j]+file_content_tokenized[i][j+1:]\n",
    "                block_file_content_tokenized[k][i][j] = \"\"\n",
    "            else:\n",
    "                j += 1\n",
    "        print(block_file_content_tokenized[k][i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 6.5: BSBI Sort and Completion\n",
    "Now that we have processed our documents in the blocks, let's just do the sorting and writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, block in enumerate(blocks):\n",
    "    in_memory_index = defaultdict(list)\n",
    "    prev_doc_id = 0\n",
    "    for j in range(len(block_file_content_tokenized[i])):\n",
    "            for term in block_file_content_tokenized[i][j]:\n",
    "                doc_id = int(os.path.basename(file_names[j]))\n",
    "                gap = doc_id - prev_doc_id\n",
    "                in_memory_index[term].append(gap)\n",
    "                prev_doc_id = doc_id\n",
    "\n",
    "    # Sort the terms in memory\n",
    "    sorted_terms = sorted(in_memory_index.keys())\n",
    "\n",
    "    # Write to Disk\n",
    "    write_block_to_disk(sorted_terms, in_memory_index, i)\n",
    "\n",
    "# Merge All\n",
    "merged_index = merge_blocks_on_disk()\n",
    "\n",
    "# Write the Merged Data to Disk\n",
    "write_merged_index_to_disk(merged_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
